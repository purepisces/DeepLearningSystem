## Question 0

This homework builds off of Homework 1. First, in your Homework 2 directory, copy the files `python/needle/autograd.py`, `python/needle/ops/ops_mathematic.py` from your Homework 1.

***NOTE***: The default data type for the tensor is `float32`. If you want to change the data type, you can do so by setting the `dtype` parameter in the `Tensor` constructor. For example, `Tensor([1, 2, 3], dtype='float64')` will create a tensor with `float64` data type. 
In this homework, make sure any tensor you create has `float32` data type to avoid any issues with the autograder.


## Question 1

  

In this first question, you will implement a few different methods for weight initialization. This will be done in the `python/needle/init/init_initializers.py` file, which contains a number of routines for initializing needle Tensors using various random and constant initializations. Following the same methodology of the existing initializers (you will want to call e.g. `init.rand` or `init.randn` implemented in `python/needle/init/init_basic.py` from your functions below, implement the following common initialization methods. In all cases, the functions should return `fan_in` by `fan_out` 2D tensors (extensions to other sizes can be done via e.g., reshaping).

  
  

### Xavier uniform

`xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs)`

  

Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{U}(-a, a)$ where

$$a = \text{gain} \times \sqrt{\frac{6}{\text{fanin} + \text{fanout}}}$$

Pass remaining `**kwargs` parameters to the corresponding `init` random call.

  
##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `gain` - optional scaling factor

**Code implementation**
```python
def xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs):
    a = gain * math.sqrt(6.0 / (fan_in + fan_out))
    return rand(fan_in, fan_out, low=-a, high=a, **kwargs)
```

### My Explanation

#### Understand rand function

The rand function is designed to generate a tensor filled with random numbers drawn from a uniform distribution over a specified range $[low, high]$. This function is particularly useful for initializing weights in neural networks or creating random data for various computational tasks.

```python
def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = ndl.cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```
**Understanding `device.rand(*shape)`**

The method `rand(*shape)` in the `CPUDevice` class is implemented(in python/needle/backend_numpy.py) as:
```python
def rand(self, *shape):
    return numpy.random.rand(*shape)
```
This method uses `numpy.random.rand(*shape)` to generate random numbers. Hereâ€™s what it does:

-   **`numpy.random.rand(*shape)`**:
    -   This function generates random numbers uniformly distributed in the interval [0,1)[0, 1)[0,1).
    -   The `*shape` argument allows you to specify the dimensions of the array (or tensor) you want to create. For example, if you pass `(3, 3)` as `shape`, it generates a 3x3 matrix where each element is a random number between 0 and 1.
    
**Understanding `device.rand(*shape) * (high - low) + low`**

```python
def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = ndl.cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```

### 1. `device.rand(*shape)`

-   **`device.rand(*shape)`** generates a tensor filled with random numbers uniformly distributed between 0 and 1.
-   The `shape` parameter specifies the dimensions of the tensor. For example, if `shape` is `(3, 3)`, this will produce a 3x3 matrix where each element is a random number between 0 and 1.

### 2. `* (high - low)`

-   The random numbers generated by `device.rand(*shape)` are in the range [0,1)[0, 1)[0,1).
-   **`high - low`** calculates the range of values you want the random numbers to cover.
    -   For example, if `low = 2.0` and `high = 5.0`, then `high - low = 3.0`.
-   Multiplying the random numbers (which are between 0 and 1) by this range scales them to be in the range [0,3)[0, 3)[0,3) in this example.

### 3. `+ low`

-   After scaling the random numbers to the desired range, you add `low` to shift the entire range to start from the `low` value.
-   Using the previous example with `low = 2.0`:
    -   After scaling, the values are in the range [0,3)[0, 3)[0,3).
    -   Adding `low` shifts this range to [2.0,5.0)[2.0, 5.0)[2.0,5.0), which is the desired range.

  **Understanding  Whole Code**
```python
def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = ndl.cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```
Parameters:

1.  ***shape**:
    
    -   This represents the dimensions of the tensor you want to create. For example, if you want a 3x3 matrix, you would pass `3, 3` as the shape.
2.  **low** (default = 0.0):
    
    -   The lower bound of the uniform distribution. The generated random numbers will be greater than or equal to this value.
3.  **high** (default = 1.0):
    
    -   The upper bound of the uniform distribution. The generated random numbers will be less than this value.
4.  **device** (default = `None`):
    
    -   Specifies the device (e.g., CPU or GPU) on which the tensor will be created. If `None`, it defaults to the CPU. The device is determined using `ndl.cpu()` unless another device (like a GPU) is specified.
5.  **dtype** (default = `"float32"`):
    
    -   Specifies the data type of the tensor. The default is `float32`, but other types like `float64`, `int32`, etc., could be specified depending on the need.
6.  **requires_grad** (default = `False`):
    
    -   A flag indicating whether the tensor should track gradients. This is useful when you need to compute gradients during backpropagation in neural networks.

Example
```python
# Create a 3x3 tensor with random values between 0 and 1
tensor1 = rand(3, 3)

# Create a 2x4 tensor with random values between -1 and 1
tensor2 = rand(2, 4, low=-1.0, high=1.0)

# Create a 5x5 tensor with random values between 0 and 10 on the GPU
tensor3 = rand(5, 5, low=0.0, high=10.0, device=ndl.gpu())

# Create a 3x3 tensor with random values between 0 and 1, requiring gradients
tensor4 = rand(3, 3, requires_grad=True)
```
#### Understand Xavier uniform

Xavier Uniform Initialization

**Concept**:

-   Xavier Uniform Initialization is a weight initialization method designed to keep the variance of the activations consistent across layers of a neural network. It was proposed by Glorot and Bengio in their 2010 paper, _"Understanding the difficulty of training deep feedforward neural networks."_

**Why It's Used**:

-   When training deep neural networks, it's essential to prevent the gradients from either vanishing (becoming too small) or exploding (becoming too large) as they are propagated back through the layers. The Xavier initialization helps in achieving this by setting the weights in such a way that the variance of the activations remains roughly the same from layer to layer.

**Mathematical Formula**:

The weights are initialized using a uniform distribution with a range from $-a$ to $a$, where $a$ is computed as:

$$a = \text{gain} \times \sqrt{\frac{6}{\text{fanin} + \text{fanout}}}$$

- **fan\_in**: The number of input units in the layer.
- **fan\_out**: The number of output units in the layer.
- **gain**: An optional scaling factor. By default, it's set to 1.0, but it can be adjusted based on specific needs (e.g., for certain activation functions).

```python
def xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs):
    a = gain * math.sqrt(6.0 / (fan_in + fan_out))
    return rand(fan_in, fan_out, low=-a, high=a, **kwargs)
```
Parameters:

-   **fan_in**: Refers to the number of input neurons feeding into the layer.
-   **fan_out**: Refers to the number of output neurons in the layer.
-   **gain**: An optional parameter that allows for scaling the range of the initialized weights. If no specific value is provided, the default gain of 1.0 is used.
-   **kwargs**: Additional keyword arguments passed to the `rand` function, such as the device on which the tensor should be created or whether the tensor should require gradients.

> ### How `**kwargs` Works:
> In Python, `**kwargs` is used to pass a variable number of keyword arguments to a function. The term "kwargs" stands for "keyword arguments," and the `**` syntax allows you to pass a dictionary of arguments to a function, where the keys are the argument names and the values are the corresponding argument values.
>
>1.  **Flexibility**: It allows a function to accept any number of keyword arguments, providing flexibility in how the function is called.
>2.  **Dictionary-Like**: Inside the function, `kwargs` behaves like a dictionary, so you can access the arguments using standard dictionary methods.
>
>Example
```python
def example_function(**kwargs):
    for key, value in kwargs.items():
        print(f"{key} = {value}")

# Calling the function with different keyword arguments
example_function(name="Alice", age=30, job="Engineer")
```
>Output
```python
name = Alice
age = 30
job = Engineer
```


___

  

### Xavier normal

`xavier_normal(fan_in, fan_out, gain=1.0, **kwargs)`

Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a normal distribution. The resulting Tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where

$$\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fanin} + \text{fanout}}}$$

##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `gain` - optional scaling factor

___

  

### Kaiming uniform

`kaiming_uniform(fan_in, fan_out, nonlinearity="relu", **kwargs)`


Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{U}(-\text{bound}, \text{bound})$ where


$$\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fanin}}}$$

Use the recommended gain value for ReLU: $\text{gain}=\sqrt{2}$.

##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `nonlinearity` - the non-linear function

___

  
### Kaiming normal

`kaiming_normal(fan_in, fan_out, nonlinearity="relu", **kwargs)`

  
Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where


$$\text{std} = \frac{\text{gain}}{\sqrt{\text{fanin}}}$$

Use the recommended gain value for ReLU: $\text{gain}=\sqrt{2}$.

  
##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `nonlinearity` - the non-linear function

