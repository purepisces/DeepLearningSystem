## Question 0

This homework builds off of Homework 1. First, in your Homework 2 directory, copy the files `python/needle/autograd.py`, `python/needle/ops/ops_mathematic.py` from your Homework 1.

***NOTE***: The default data type for the tensor is `float32`. If you want to change the data type, you can do so by setting the `dtype` parameter in the `Tensor` constructor. For example, `Tensor([1, 2, 3], dtype='float64')` will create a tensor with `float64` data type. 
In this homework, make sure any tensor you create has `float32` data type to avoid any issues with the autograder.


## Question 1

  

In this first question, you will implement a few different methods for weight initialization. This will be done in the `python/needle/init/init_initializers.py` file, which contains a number of routines for initializing needle Tensors using various random and constant initializations. Following the same methodology of the existing initializers (you will want to call e.g. `init.rand` or `init.randn` implemented in `python/needle/init/init_basic.py` from your functions below, implement the following common initialization methods. In all cases, the functions should return `fan_in` by `fan_out` 2D tensors (extensions to other sizes can be done via e.g., reshaping).

  
  

### Xavier uniform

`xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs)`

  

Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{U}(-a, a)$ where

$$a = \text{gain} \times \sqrt{\frac{6}{\text{fanin} + \text{fanout}}}$$

Pass remaining `**kwargs` parameters to the corresponding `init` random call.

  
##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `gain` - optional scaling factor

**Code implementation**
```python
def xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs):
    ### BEGIN YOUR SOLUTION
    # Calculate the range for uniform distribution
    a = gain * math.sqrt(6.0 / (fan_in + fan_out))
    # Generate a tensor with values from the uniform distribution
    return rand(fan_in, fan_out, low=-a, high=a, **kwargs)
    ### END YOUR SOLUTION
```

### My Explanation

#### Understand rand function

The rand function is designed to generate a tensor filled with random numbers drawn from a uniform distribution over a specified range $[low, high]$. This function is particularly useful for initializing weights in neural networks or creating random data for various computational tasks.

```python
def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = ndl.cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```
**Understanding `device.rand(*shape)`**

The method `rand(*shape)` in the `CPUDevice` class is implemented(in python/needle/backend_numpy.py) as:
```python
def rand(self, *shape):
    return numpy.random.rand(*shape)
```
This method uses `numpy.random.rand(*shape)` to generate random numbers. Here’s what it does:

-   **`numpy.random.rand(*shape)`**:
    -   This function generates random numbers uniformly distributed in the interval [0,1).
    -   The `*shape` argument allows you to specify the dimensions of the array (or tensor) you want to create. For example, if you pass `(3, 3)` as `shape`, it generates a 3x3 matrix where each element is a random number between 0 and 1.
    
**Understanding `device.rand(*shape) * (high - low) + low`**

```python
def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = ndl.cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```

### 1. `device.rand(*shape)`

-   **`device.rand(*shape)`** generates a tensor filled with random numbers uniformly distributed between 0 and 1.
-   The `shape` parameter specifies the dimensions of the tensor. For example, if `shape` is `(3, 3)`, this will produce a 3x3 matrix where each element is a random number between 0 and 1.

### 2. `* (high - low)`

-   The random numbers generated by `device.rand(*shape)` are in the range [0,1).
-   **`high - low`** calculates the range of values you want the random numbers to cover.
    -   For example, if `low = 2.0` and `high = 5.0`, then `high - low = 3.0`.
-   Multiplying the random numbers (which are between 0 and 1) by this range scales them to be in the range [0,3)[0, 3)[0,3) in this example.

### 3. `+ low`

-   After scaling the random numbers to the desired range, you add `low` to shift the entire range to start from the `low` value.
-   Using the previous example with `low = 2.0`:
    -   After scaling, the values are in the range [0,3).
    -   Adding `low` shifts this range to [2.0,5.0), which is the desired range.

  **Understanding  Whole Code**
```python
def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = ndl.cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```
Parameters:

1.  ***shape**:
    
    -   This represents the dimensions of the tensor you want to create. For example, if you want a 3x3 matrix, you would pass `3, 3` as the shape.
2.  **low** (default = 0.0):
    
    -   The lower bound of the uniform distribution. The generated random numbers will be greater than or equal to this value.
3.  **high** (default = 1.0):
    
    -   The upper bound of the uniform distribution. The generated random numbers will be less than this value.
4.  **device** (default = `None`):
    
    -   Specifies the device (e.g., CPU or GPU) on which the tensor will be created. If `None`, it defaults to the CPU. The device is determined using `ndl.cpu()` unless another device (like a GPU) is specified.
5.  **dtype** (default = `"float32"`):
    
    -   Specifies the data type of the tensor. The default is `float32`, but other types like `float64`, `int32`, etc., could be specified depending on the need.
6.  **requires_grad** (default = `False`):
    
    -   A flag indicating whether the tensor should track gradients. This is useful when you need to compute gradients during backpropagation in neural networks.

Example
```python
# Create a 3x3 tensor with random values between 0 and 1
tensor1 = rand(3, 3)

# Create a 2x4 tensor with random values between -1 and 1
tensor2 = rand(2, 4, low=-1.0, high=1.0)

# Create a 5x5 tensor with random values between 0 and 10 on the GPU
tensor3 = rand(5, 5, low=0.0, high=10.0, device=ndl.gpu())

# Create a 3x3 tensor with random values between 0 and 1, requiring gradients
tensor4 = rand(3, 3, requires_grad=True)
```
#### Understand Xavier uniform

Xavier Uniform Initialization

**Concept**:

-   Xavier Uniform Initialization is a weight initialization method designed to keep the variance of the activations consistent across layers of a neural network. It was proposed by Glorot and Bengio in their 2010 paper, _"Understanding the difficulty of training deep feedforward neural networks."_

**Why It's Used**:

-   When training deep neural networks, it's essential to prevent the gradients from either vanishing (becoming too small) or exploding (becoming too large) as they are propagated back through the layers. The Xavier initialization helps in achieving this by setting the weights in such a way that the variance of the activations remains roughly the same from layer to layer.

**Mathematical Formula**:

The weights are initialized using a uniform distribution with a range from $-a$ to $a$, where $a$ is computed as:

$$a = \text{gain} \times \sqrt{\frac{6}{\text{fanin} + \text{fanout}}}$$

- **fan\_in**: The number of input units in the layer.
- **fan\_out**: The number of output units in the layer.
- **gain**: An optional scaling factor. By default, it's set to 1.0, but it can be adjusted based on specific needs (e.g., for certain activation functions).

```python
def xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs):
    ### BEGIN YOUR SOLUTION
    # Calculate the range for uniform distribution
    a = gain * math.sqrt(6.0 / (fan_in + fan_out))
    # Generate a tensor with values from the uniform distribution
    return rand(fan_in, fan_out, low=-a, high=a, **kwargs)
    ### END YOUR SOLUTION
```
Parameters:

-   **fan_in**: Refers to the number of input neurons feeding into the layer.
-   **fan_out**: Refers to the number of output neurons in the layer.
-   **gain**: An optional parameter that allows for scaling the range of the initialized weights. If no specific value is provided, the default gain of 1.0 is used.
-   **kwargs**: Additional keyword arguments passed to the `rand` function, such as the device on which the tensor should be created or whether the tensor should require gradients.

> ### How `**kwargs` Works:
> In Python, `**kwargs` is used to pass a variable number of keyword arguments to a function. The term "kwargs" stands for "keyword arguments," and the `**` syntax allows you to pass a dictionary of arguments to a function, where the keys are the argument names and the values are the corresponding argument values.
>
>1.  **Flexibility**: It allows a function to accept any number of keyword arguments, providing flexibility in how the function is called.
>2.  **Dictionary-Like**: Inside the function, `kwargs` behaves like a dictionary, so you can access the arguments using standard dictionary methods.
>
>Example
```python
def example_function(**kwargs):
    for key, value in kwargs.items():
        print(f"{key} = {value}")

# Calling the function with different keyword arguments
example_function(name="Alice", age=30, job="Engineer")
```
>Output
```python
name = Alice
age = 30
job = Engineer
```

___

  

### Xavier normal

`xavier_normal(fan_in, fan_out, gain=1.0, **kwargs)`

Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a normal distribution. The resulting Tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where

$$\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fanin} + \text{fanout}}}$$

##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `gain` - optional scaling factor


**Code implementation**
```python
def xavier_normal(fan_in, fan_out, gain=1.0, **kwargs):
    ### BEGIN YOUR SOLUTION
    # Calculate the standard deviation
    std = gain * math.sqrt(2.0 / (fan_in + fan_out))
    # Generate a tensor with the normal distribution
    return randn(fan_in, fan_out, mean=0.0, std=std, **kwargs)
    ### END YOUR SOLUTION
```

### My Explanation

#### Understand randn function

The `randn` function is designed to generate a tensor filled with random numbers drawn from a normal (Gaussian) distribution with a specified mean and standard deviation. This function is particularly useful for initializing weights in neural networks, where a normal distribution is often used to ensure that the weights are centered around a certain value (mean) and spread out according to the standard deviation.

```python
def randn(*shape, mean=0.0, std=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random normal with specified mean and std deviation"""
    device = ndl.cpu() if device is None else device
    array = device.randn(*shape) * std + mean
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```

**Parameters**:

-   `*shape`: Specifies the dimensions of the tensor to be generated. For example, passing `(3, 3)` would generate a 3x3 tensor.
-   `mean` (default = 0.0): The mean of the normal distribution. This value is the center of the distribution, where most of the random values will cluster.
-   `std` (default = 1.0): The standard deviation of the normal distribution. This value controls the spread of the distribution; a larger `std` means the values will be more spread out from the mean.
-   `device` (default = `None`): Specifies the device (CPU or GPU) on which the tensor will be created. If `None`, it defaults to the CPU.
-   `dtype` (default = `"float32"`): Specifies the data type of the tensor. The default is `float32`, but other types like `float64` can be specified depending on the need.
-   `requires_grad` (default = `False`): A flag indicating whether the tensor should track gradients. This is useful when you need to compute gradients during backpropagation in neural networks.

**Understanding `device.randn(*shape)`**
The method `randn(*shape)` in the `CPUDevice` class is implemented(in python/needle/backend_numpy.py) as:
```python
def  randn(self, *shape):
	# note: numpy doesn't support types within standard random routines, and
	# .astype("float32") does work if we're generating a singleton
	return  numpy.random.randn(*shape)
```
This method uses `numpy.random.randn(*shape)` to generate random numbers. Here’s what it does:

-   **`numpy.random.rand(*shape)`**:
    -   This function generates random numbers drawn from a normal (Gaussian) distribution with a mean of 0 and a standard deviation of 1.
    -   The `*shape` argument allows you to specify the dimensions of the array (or tensor) you want to create. For example, if you pass `(3, 3)` as `shape`, it generates a 3x3 matrix where each element is a random number sampled from the standard normal distribution (mean = 0, std = 1).
    
**Understanding `device.randn(*shape) * std + mean`**

```python
def randn(*shape, mean=0.0, std=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random normal with specified mean and std deviation"""
    device = ndl.cpu() if device is None else device
    array = device.randn(*shape) * std + mean
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```

### 1. `device.randn(*shape)`

-   **`device.randn(*shape)`** generates a tensor filled with random numbers drawn from a standard normal distribution with a mean of 0 and a standard deviation of 1.
-   The `shape` parameter specifies the dimensions of the tensor. For example, if `shape` is `(3, 3)`, this will produce a 3x3 matrix where each element is a random number drawn from the standard normal distribution (i.e., mean = 0, std = 1).

### 2. `* std`

-   The random numbers generated by `device.randn(*shape)` are initially distributed with a mean of 0 and a standard deviation of 1.
-   **`std`** is the desired standard deviation for the distribution of the random numbers.
    -   For example, if `std = 2.0`, this multiplication scales the standard deviation of the random numbers from 1 to 2.
-   Multiplying the random numbers by `std` scales them to have the specified standard deviation. For instance, if `std = 2.0`, the distribution of the random numbers will now have a standard deviation of 2 instead of 1.

### 3. `+ mean`

-   After scaling the random numbers to have the desired standard deviation, you add `mean` to shift the entire distribution to center around the specified mean.
-   Using the previous example with `mean = 5.0`:
    -   After scaling, the random numbers are still centered around 0 (since the initial distribution had a mean of 0).
    -   Adding `mean` shifts the distribution so that the random numbers are now centered around 5.0.
-   This operation ensures that the final tensor has random numbers distributed with the specified mean (`mean`) and standard deviation (`std`). For instance, if `mean = 5.0` and `std = 2.0`, the resulting numbers will be centered around 5.0 with most values falling within 2.0 units of this mean.

  **Understanding  Whole Code**
```python
def randn(*shape, mean=0.0, std=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random normal with specified mean and std deviation"""
    device = ndl.cpu() if device is None else device
    array = device.randn(*shape) * std + mean
    return ndl.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)
```
#### Parameters:

1.  ***shape**:
    -   This represents the dimensions of the tensor you want to create. For example, if you want a 3x3 matrix, you would pass `3, 3` as the shape.
2.  **mean** (default = 0.0):
    -   The mean of the normal distribution. The generated random numbers will be centered around this value.
3.  **std** (default = 1.0):
    -   The standard deviation of the normal distribution. This controls the spread of the generated random numbers around the mean. A higher `std` results in more spread out values.
4.  **device** (default = `None`):
    -   Specifies the device (e.g., CPU or GPU) on which the tensor will be created. If `None`, it defaults to the CPU. The device is determined using `ndl.cpu()` unless another device (like a GPU) is specified.
5.  **dtype** (default = `"float32"`):
    -   Specifies the data type of the tensor. The default is `float32`, but other types like `float64`, `int32`, etc., could be specified depending on the need.
6.  **requires_grad** (default = `False`):
    -   A flag indicating whether the tensor should track gradients. This is useful when you need to compute gradients during backpropagation in neural networks.

Example
```python
# Create a 3x3 tensor with random values from a normal distribution with mean 0 and std 1
tensor1 = randn(3, 3)

# Create a 2x4 tensor with random values from a normal distribution with mean 5 and std 2
tensor2 = randn(2, 4, mean=5.0, std=2.0)

# Create a 5x5 tensor with random values from a normal distribution with mean 0 and std 10 on the GPU
tensor3 = randn(5, 5, mean=0.0, std=10.0, device=ndl.gpu())

# Create a 3x3 tensor with random values from a normal distribution with mean 0 and std 1, requiring gradients
tensor4 = randn(3, 3, requires_grad=True)
```
#### Understand Xavier Normal

Xavier Normal Initialization

**Concept**:

-   Xavier Normal Initialization is a weight initialization method designed to keep the variance of the activations consistent across layers of a neural network. It was proposed by Glorot and Bengio in their 2010 paper, _"Understanding the difficulty of training deep feedforward neural networks."_
- Unlike Xavier Uniform Initialization, which uses a uniform distribution, Xavier Normal Initialization uses a normal (Gaussian) distribution to initialize the weights. This method is particularly useful for ensuring that the network starts with weights that prevent the gradients from either vanishing or exploding during backpropagation

**Why It's Used**:

- When training deep neural networks, it’s crucial to maintain the stability of gradients as they are propagated back through the network. Xavier Normal Initialization helps achieve this by initializing the weights in such a way that the variance of the activations remains consistent across layers. This is particularly important for networks using activation functions like sigmoid or tanh.

**Mathematical Formula**:

The weights are initialized using a uniform distribution with a mean of 0 and a standard deviation(std) computed as:

$$\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fanin} + \text{fanout}}}$$

-   **fan_in**: The number of input units in the layer.
-   **fan_out**: The number of output units in the layer.
-   **gain**: An optional scaling factor. By default, it's set to 1.0, but it can be adjusted based on specific needs (e.g., for certain activation functions).

```python
def xavier_normal(fan_in, fan_out, gain=1.0, **kwargs):
    # Calculate the standard deviation
    std = gain * math.sqrt(2.0 / (fan_in + fan_out))
    # Generate a tensor with the normal distribution
    return randn(fan_in, fan_out, mean=0.0, std=std, **kwargs)
```
Parameters:

-   **fan_in**: Refers to the number of input neurons feeding into the layer.
-   **fan_out**: Refers to the number of output neurons in the layer.
-   **gain**: An optional parameter that allows for scaling the standard deviation of the initialized weights. If no specific value is provided, the default gain of 1.0 is used.
-   **kwargs**: Additional keyword arguments passed to the `randn` function, such as the device on which the tensor should be created or whether the tensor should require gradients.


___

  

### Kaiming uniform

`kaiming_uniform(fan_in, fan_out, nonlinearity="relu", **kwargs)`


Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{U}(-\text{bound}, \text{bound})$ where


$$\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fanin}}}$$

Use the recommended gain value for ReLU: $\text{gain}=\sqrt{2}$.

##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `nonlinearity` - the non-linear function

**Code implementation**
```python
def kaiming_uniform(fan_in, fan_out, nonlinearity="relu", **kwargs):
    assert nonlinearity == "relu", "Only relu supported currently"
    ### BEGIN YOUR SOLUTION
    # Use the recommended gain value for ReLU: gain = sqrt(2)
    gain = math.sqrt(2.0)
    
    # Calculate the bound for the uniform distribution
    bound = gain * math.sqrt(3.0 / fan_in)
    
    # Generate and return a tensor with values uniformly distributed between -bound and bound
    return rand(fan_in, fan_out, low=-bound, high=bound, **kwargs)
    ### END YOUR SOLUTION
```

### My Explanation

#### Understand Kaiming Uniform

Kaiming Uniform Initialization

**Concept**:

-   Kaiming Uniform Initialization is a weight initialization method specifically designed for deep neural networks using ReLU (Rectified Linear Unit) or similar activation functions. It was introduced by He et al. in their 2015 paper, _"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification."_
- Unlike Xavier Initialization, which is designed for activations that can output both positive and negative values (like sigmoid or tanh), Kaiming Initialization accounts for the fact that ReLU activations only output positive values. This method helps to maintain the variance of the activations throughout the layers, preventing issues like vanishing or exploding gradients during training.

**Why It's Used**:

- When training deep neural networks with ReLU activations, it's essential to initialize the weights in such a way that the variance of the activations remains stable as they propagate through the layers. Kaiming Uniform Initialization helps achieve this by selecting the weights from a uniform distribution with bounds that are tailored to the properties of ReLU activations. This ensures that the gradients remain well-behaved, facilitating efficient and stable training.

**Mathematical Formula**:

The weights are initialized using a uniform distribution with a range from -bound to bound, where bound is computed as:

$$\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fanin}}}$$

-   **fan_in**: The number of input units in the layer.
-   **fan_out**: The number of output units in the layer.
-   **gain**: A scaling factor that accounts for the properties of the ReLU activation function. For ReLU, the recommended value is $\sqrt{2}$.

```python
def kaiming_uniform(fan_in, fan_out, nonlinearity="relu", **kwargs):
    assert nonlinearity == "relu", "Only relu supported currently"
    ### BEGIN YOUR SOLUTION
    # Use the recommended gain value for ReLU: gain = sqrt(2)
    gain = math.sqrt(2.0)
    
    # Calculate the bound for the uniform distribution
    bound = gain * math.sqrt(3.0 / fan_in)
    
    # Generate and return a tensor with values uniformly distributed between -bound and bound
    return rand(fan_in, fan_out, low=-bound, high=bound, **kwargs)
    ### END YOUR SOLUTION
```

### Parameters:

-   **fan_in**: Refers to the number of input neurons feeding into the layer.
-   **fan_out**: Refers to the number of output neurons in the layer.
-   **nonlinearity**: The non-linear function used in the network. This implementation currently supports only ReLU.
-   **gain**: A scaling factor that adjusts the range of the uniform distribution. For ReLU, the recommended gain is $\sqrt{2}$.
-   **kwargs**: Additional keyword arguments passed to the `rand` function, such as the device on which the tensor should be created or whether the tensor should require gradients.

> Kaiming initialization was introduced specifically to handle the challenges posed by ReLU activation functions in deep neural networks. While Xavier initialization works well for activation functions that output both positive and negative values symmetrically, it does not optimally handle the characteristics of ReLU. Kaiming initialization adjusts for these characteristics, providing a more effective weight initialization strategy that helps maintain variance and improve training stability, especially in deeper networks.
___

  
### Kaiming normal

`kaiming_normal(fan_in, fan_out, nonlinearity="relu", **kwargs)`

  
Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where


$$\text{std} = \frac{\text{gain}}{\sqrt{\text{fanin}}}$$

Use the recommended gain value for ReLU: $\text{gain}=\sqrt{2}$.

  
##### Parameters

- `fan_in` - dimensionality of input

- `fan_out` - dimensionality of output

- `nonlinearity` - the non-linear function


**Code implementation**
```python
def kaiming_normal(fan_in, fan_out, nonlinearity="relu", **kwargs):
    assert nonlinearity == "relu", "Only relu supported currently"
    ### BEGIN YOUR SOLUTION
    # Use the recommended gain value for ReLU: gain = sqrt(2)
    gain = math.sqrt(2.0)
    
    # Calculate the standard deviation for the normal distribution
    std = gain / math.sqrt(fan_in)
    
    # Generate and return a tensor with values from the normal distribution
    return randn(fan_in, fan_out, mean=0.0, std=std, **kwargs)
    ### END YOUR SOLUTION
```

### My Explanation

#### Understand Kaiming Normal

Kaiming Normal Initialization

**Concept**:

-   Kaiming Normal Initialization is a weight initialization method specifically designed for deep neural networks using ReLU (Rectified Linear Unit) or similar activation functions. It was introduced by He et al. in their 2015 paper, _"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification."_
-   Unlike Xavier Initialization, which is designed for activations that can output both positive and negative values (like sigmoid or tanh), Kaiming Initialization accounts for the fact that ReLU activations only output positive values. This method helps to maintain the variance of the activations throughout the layers, preventing issues like vanishing or exploding gradients during training.

**Why It's Used**:

- When training deep neural networks with ReLU activations, it's crucial to initialize the weights in such a way that the variance of the activations remains stable as they propagate through the layers. Kaiming Normal Initialization helps achieve this by selecting the weights from a normal distribution with a standard deviation that is tailored to the properties of ReLU activations. This ensures that the gradients remain well-behaved, facilitating efficient and stable training.

**Mathematical Formula**:

The weights are initialized using a normal distribution with a mean of 0 and a standard deviation (std) computed as::

$$\text{std} = \frac{\text{gain}}{\sqrt{\text{fanin}}}$$

-   **fan_in**: The number of input units in the layer.
-   **fan_out**: The number of output units in the layer.
-   **gain**: A scaling factor that accounts for the properties of the ReLU activation function. For ReLU, the recommended value is $\sqrt{2}$.

```python
def kaiming_normal(fan_in, fan_out, nonlinearity="relu", **kwargs):
    assert nonlinearity == "relu", "Only relu supported currently"
    ### BEGIN YOUR SOLUTION
    # Use the recommended gain value for ReLU: gain = sqrt(2)
    gain = math.sqrt(2.0)
    
    # Calculate the standard deviation for the normal distribution
    std = gain / math.sqrt(fan_in)
    
    # Generate and return a tensor with values from the normal distribution
    return randn(fan_in, fan_out, mean=0.0, std=std, **kwargs)
    ### END YOUR SOLUTION
```


### Parameters:

-   **fan_in**: Refers to the number of input neurons feeding into the layer.
-   **fan_out**: Refers to the number of output neurons in the layer.
-   **nonlinearity**: The non-linear function used in the network. This implementation currently supports only ReLU.
-   **gain**: A scaling factor that adjusts the standard deviation of the normal distribution. For ReLU, the recommended gain is $\sqrt{2}$.
-   **kwargs**: Additional keyword arguments passed to the `randn` function, such as the device on which the tensor should be created or whether the tensor should require gradients.


