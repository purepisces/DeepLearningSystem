**I still haven't finish question 6, the c++ part.**

Hw0 includes functions to read and parse the MNIST dataset, compute softmax loss, and perform Stochastic Gradient Descent (SGD) for both softmax regression and a simple two-layer neural network, providing a foundational framework for the MNIST digit classification problem.

- **hw0.ipynb**: The original Jupyter notebook.
- **hw0_part1.md**: homework explanation
- **hw0_part2.md**: homework explanation
- **hw0_tech.md**: homework python technical
- **hw0_original**: This folder contains the original homework without solutions. It includes the initial code and the original notebook to help you get started. **You just need the hw0.zip and hw0.iqynb to get start from scratch.**
- **hw0_solution**: Completed solution.
- **manual_neural_nets.pdf**: Slides in course.
- **softmax_regression.pdf**: Slides in course.

